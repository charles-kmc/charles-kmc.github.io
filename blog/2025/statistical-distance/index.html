<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Common statistical ways to measure distance between two distributions | Charlesquin Kemajou Mbakam </title> <meta name="author" content="Charlesquin Kemajou Mbakam"> <meta name="description" content=""> <meta name="keywords" content="Deep learning, Diffusion model, Bayesian model, Computional imaging"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://charles-kmc.github.io/blog/2025/statistical-distance/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Charlesquin</span> Kemajou Mbakam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Common statistical ways to measure distance between two distributions</h1> <p class="post-meta"> Created in February 17, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/generative-models"> <i class="fa-solid fa-tag fa-sm"></i> generative-models</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3> Introduction </h3> <p>In this block, we explore key distances and metrics between distributions that are essential in statistics. Evaluating distance between two distributions has become ubiqutous task in various research fields and applications. For instance, in generative adversarial network (GAN), the primary objective is to minimize the distance between the training distribution and the generated distribution. Another example is independence testing, where we are given samples \((X_1, Y_1), \dots, (X_n, Y_n) \sim P_{XY}\) and we want test the distance between the joint distribution \(P_{XY}\) and the product of marginal distributions \(P_XP_Y\). We discuss several important measures including total variation, KL divergence, maximum mean discrepancy, f-divergence and wasserstein distance.</p> <p>Before delving into the details, let’s first consider a measurable space \(\mathcal{M}=\left(\Omega, \mathcal{F}\right)\) and two probability distributions \(P\) and \(Q\) defined of \(\mathcal{M}\). We say \(Q\) dominate \(P\) when the following statement hold: If \(Q(A)=0\) for some set \(A\) then it has to be the case that \(P(A)\) is also \(0\).</p> <h3> Total variation (TV) distance </h3> <p>Total variation distance is a statistical distance between two probability distributions. Mathematically, it is defined as follows</p> <p>\begin{equation} \text{TV}(P,Q) = \sup_{A\in\mathcal{F}}|P(A) - Q(A)|, \end{equation}</p> <p>where \(A\) is a measurable set. Precisely, total variation distance is the largest absolute distance between probabilities that the two probability distributions assign to the same event. In other word, it measures the maximum difference between the probability of an event under \(p\) and under \(Q\).</p> <p>When \(P\) and \(Q\) are associated with probability densities \(p\) and \(q\) respectively, the total variation distance become,</p> <p>\begin{equation}\label{eq:tv2} \text{TV}(P,Q)= \frac{1}{2}\int_{x \in A} |p(x) - q(x)|dx. \end{equation}</p> <p>Notice that \eqref{eq:tv2} reveals that TV distance is equivant the \(\ell_1\) distance between densities.</p> <h3> Kullback-Leibler (KL) divergence </h3> <p>Introduced by Solomon Kullback and Richard Leibler, KL divergence (also called relative entropy) is a statistical distance that measure how much a model probability \(P\) is different from a true probability distribution \(Q\). For continuous probability distribution \(P\) and \(Q\), the KL divergence is given by,</p> <p>\begin{equation} \text{D}_{KL}(P,Q) = \int p(x) \log \left(\dfrac{p(x)}{q(x)}\right)dx. \end{equation}</p> <p>For discrete probability distribution \(P\) and \(Q\), the KL divergence is given by,</p> <p>\begin{equation} \text{D}<em>{KL}(P,Q) = \sum</em>{x} p(x) \log \left(\dfrac{p(x)}{q(x)}\right)dx. \end{equation}</p> <p>KL divergence if not symmetric \(\text{D}_{KL}(P,Q) \neq \text{D}_{KL}(Q,P)\) and does not statisfy the triangle inequality. Consequently, KL divergence is not a metric.</p> <p>In real application, KL divergence is used when one want to approximation a complex true distribution \(P\) with a simple approximate \(Q\) which is most case is Gaussian. This technique is quite used in situation where it is easier to compute, such as expectation-maximazation algorithm (EM) and evidence lower bound (ELBO) computations.</p> <p>Here we have some properties of KL divergence.</p> <ul> <li>Consider two multivariate Gaussian distributions \(P_1=\mathcal{N}(\mu_1, \Sigma_1)\) and \(P_2=\mathcal{N}(\mu_2, \Sigma_2)\), the KL deivergence between \(P_1\) and \(P_2\) has a close form expression given by</li> </ul> <p>\begin{equation} \text{D}_{KL}(P_1, P_2) = \frac{1}{2}\left(\text{tr}(\Sigma^{-1}_2\Sigma_1) -d + (\mu_2-\mu_1)^\top\Sigma_2^{-1}(\mu_2-\mu_1) + \ln\left(\dfrac{\text{det}\Sigma_2}{\text{det}\Sigma_1}\right)\right) \end{equation}</p> <ul> <li>Consider two uniform distributions \(P_1=\mathcal{U}(A, B)\) and \(P_2=\mathcal{U}(C,D)\), the KL deivergence between \(P_1\) and \(P_2\) has a close form expression given by</li> </ul> <p>\begin{equation} \text{D}_{KL}(P_1, P_2) =\log\left(\dfrac{D-C}{B-A}\right). \end{equation}</p> <h3> Wasserstein distance </h3> <p>Named after Leonid Vaserstein, wasserstein distance also named Kantorovich-Rubinstein metric is a distance nction between probability distributions on a given metric spaces \(X\).</p> <p>Let condider \(\left(M, d\right)\) be a metric space. For \(p\in\left[1, +\infty\right]\), the Wasserstein \(p-\)distance between two probability measures \(\mu\) and \(\nu\) on \(M\) is given by</p> <p>\begin{equation} W_p(\mu,\nu) = \inf_{\gamma \in \Gamma(\mu,\nu)} \left(\mathbb{E}_{x,y \sim \gamma}d(x,y)^p\right)^{\frac{1}{p}}, \end{equation}</p> <p>where \(\Gamma(\mu,\nu)\) is the set of couplings (joint probability) of \(\mu\) (marginal) and \(\nu\) (marginal).</p> <p>When \(P\) and \(Q\) are empirical distributions with samples \(X_1, \dots,X_n\) and \(Y_1,\dots,Y_n\) respectively, the wasserstein \(p-\)distance is given by</p> <p>\begin{equation} W_p(P,Q) = \left(\frac{1}{n}\sum_{i=1}^n||X_i-Y_i||^p\right)^{\frac{1}{p}}. \end{equation}</p> <p>In practical application, wesserstein distance is a natural way to compare probability distributions of two random variables \(X\) and \(Y\), where one variable is deviated from the other by small, non-uniform pertubations.</p> <h3> Maximum Mean Discrepancy (MMD) </h3> <p>MMD is a distance on the space of probability measures which has found numerous applications in machine learning and non-parametric testing. The distance is based on the notion of embedding probabilities in a reproducing kernel hilbert space (RKHS). This means each probability distribution is represented as an element of th RKHS. The key point is to embedded probability distributions into an infinite-dimensional features spaces, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis.</p> <p>Let \(\mathcal{F}_k\) to be a unit ball in a reproducing kernel hilbert space, where \(k\) is a kernel measuring similarity. The MMD between \(P\) and \(Q\) is defened as follows:</p> <p>\begin{equation} \text{MMD}(P,Q) = \sup_{f\in\mathcal{F}<em>{k}}|\mathbb{E}</em>{X\sim P}\left[f(X)\right] - \mathbb{E}_{Y\sim Q}\left[f(Y)\right]|. \end{equation}</p> <p>This distance can be rewritten as follows</p> <p>\begin{equation} \text{MMD}(P,Q) = \mathbb{E}<em>{X,X’\sim P}k(X,X’) - 2\mathbb{E}</em>{Y\sim Q, X\sim P}k(X,Y)+ \mathbb{E}_{Y,Y’\sim P}k(Y,Y’). \end{equation}</p> <p>In practical application, the MMD measure is quite used because it is simple to evaluate. Notice that MMD measure consist of expectation values for which wa can use empirical expectation.</p> <h3> Jensen-Shannon divergence </h3> <h3> f-divergence </h3> <p>— –&gt;</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/machine-learning-resources/">Machine learning resources</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Charlesquin Kemajou Mbakam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-talks",title:"talks",description:"",section:"Navigation",handler:()=>{window.location.href="/talks/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-people",title:"people",description:"",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"cv",description:"Let Make the change we want to see in the world.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-common-statistical-ways-to-measure-distance-between-two-distributions",title:"Common statistical ways to measure distance between two distributions",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/statistical-distance/"}},{id:"post-machine-learning-resources",title:"Machine learning resources",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/machine-learning-resources/"}},{id:"news-i-successfully-passed-my-phd-viva-with-prof-audrey-repetti-https-researchportal-hw-ac-uk-en-persons-audrey-repetti-as-the-internal-examiner-and-prof-martin-benning-https-profiles-ucl-ac-uk-95169-martin-benning-about-as-the-external-examiner",title:"I successfully passed my PhD viva, with [Prof. Audrey Repetti](https://researchportal.hw.ac.uk/en/persons/audrey-repetti) as the internal...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/conditional_diffusion_model/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%6B%65%6D%61%6A%6F%75@%61%69%6D%73%61%6D%6D%69.%6F%72%67","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=UBwXShAAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/charles-kmc","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>